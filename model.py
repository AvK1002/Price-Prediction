# -*- coding: utf-8 -*-
"""Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQyBRAQV7OlAaVkv_C81izKRzC6CrYDW
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV # type: ignore
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error # type: ignore
from sklearn.preprocessing import StandardScaler # type: ignore
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor # type: ignore
from sklearn.linear_model import Ridge, Lasso # type: ignore
from xgboost import XGBRegressor
from statsmodels.stats.outliers_influence import variance_inflation_factor # type: ignore
import warnings

warnings.filterwarnings('ignore')

# Step 1: Data Loading and Cleaning
def load_and_clean_data(filepath):
    data = pd.read_csv('/content/Real Estate Property Transactions Dataset.csv')
    data.dropna(subset=['Estimated Value', 'carpet_area'], inplace=True)
    data['Locality'].fillna('Unknown', inplace=True)
    data['Residential'].fillna('Unknown', inplace=True)
    data.drop_duplicates(inplace=True)

    # prompt: drop column Date , Face and Property

    data = data.drop(columns=['Date', 'Face', 'Property'], axis=1)

    # Remove outliers using Z-score
    z_scores = np.abs((data[['Estimated Value', 'Sale Price', 'carpet_area']] -
                      data[['Estimated Value', 'Sale Price', 'carpet_area']].mean()) /
                      data[['Estimated Value', 'Sale Price', 'carpet_area']].std())
    data = data[(z_scores < 3).all(axis=1)]
    # Drop rows with extreme 'Estimated Value'
    data = data[data['Estimated Value'] >= 3499]
    return data

# Step 2: Encoding Categorical Features
def encode_features(data):
    encoding_mappings = {
        'Locality': {'Bridgeport': 0, 'Waterbury': 1, 'Fairfield': 2, 'West Hartford': 3,
                     'Greenwich': 4, 'Norwalk': 5, 'Stamford': 6, 'Unknown': 7},
        'Residential': {'Detached House': 0, 'Duplex': 1, 'Triplex': 2, 'Fourplex': 3, 'Unknown': 4}
    }
    for col, mapping in encoding_mappings.items():
        data[col] = data[col].map(mapping)
    return data

# Step 3: Calculate and Reduce VIF
def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Feature"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def reduce_multicollinearity(X, vif_threshold=10, essential_features=None):
    if essential_features is None:
        essential_features = []  # Default to no essential features

    while True:
        vif_data = calculate_vif(X)
        max_vif = vif_data["VIF"].max()

        if max_vif > vif_threshold:
            # Find the feature with the highest VIF
            feature_to_drop = vif_data.sort_values("VIF", ascending=False).iloc[0]["Feature"]

            # Check if the feature is essential and should not be dropped
            if feature_to_drop not in essential_features:
                X = X.drop(columns=[feature_to_drop])
                print(f"Dropped {feature_to_drop} due to high VIF.")
            else:
                print(f"{feature_to_drop} is essential, skipping the drop.")
                break  # Stop if the highest VIF feature is essential
        else:
            break

    return X


# Step 4: Model Training and Evaluation
def train_and_evaluate(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Define models
    models = {
        'RandomForest': RandomForestRegressor(),
        'GradientBoosting': GradientBoostingRegressor(),
        'XGBoost': XGBRegressor(),
        'Ridge': Ridge(),
        'Lasso': Lasso()
    }

    # Hyperparameter tuning with GridSearchCV for the best model
    param_grid = {
        'XGBoost': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0]
        }
    }

    best_models = {}
    for name, model in models.items():
        if name in param_grid:
            grid_search = GridSearchCV(model, param_grid[name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
            grid_search.fit(X_train_scaled, y_train)
            best_models[name] = grid_search.best_estimator_
            print(f"Best {name} Model: {grid_search.best_params_}")
        else:
            model.fit(X_train_scaled, y_train)
            best_models[name] = model

        # Evaluate on the test set
        y_pred = best_models[name].predict(X_test_scaled)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"{name} - R2: {r2:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

    # Stacking Model
    stacking_model = StackingRegressor(
        estimators=[('rf', best_models['RandomForest']),
                    ('gb', best_models['GradientBoosting']),
                    ('xgb', best_models['XGBoost'])],
        final_estimator=Lasso()
    )
    stacking_model.fit(X_train_scaled, y_train)
    y_pred_stack = stacking_model.predict(X_test_scaled)

    mse_stack = mean_squared_error(y_test, y_pred_stack)
    rmse_stack = np.sqrt(mse_stack)
    mae_stack = mean_absolute_error(y_test, y_pred_stack)
    r2_stack = r2_score(y_test, y_pred_stack)

    print(f"Stacking Model - R2: {r2_stack:.4f}, MSE: {mse_stack:.4f}, RMSE: {rmse_stack:.4f}, MAE: {mae_stack:.4f}")
    return stacking_model

# Load and preprocess data
data = load_and_clean_data('/path/to/Real Estate Property Transactions Dataset.csv')
data = encode_features(data)

# Updated feature set
features = ['Estimated Value', 'Locality', 'num_bathrooms', 'Residential', 'property_tax_rate']
target = 'Sale Price'
X = data[features]
y = data[target]

# Reduce multicollinearity
X = reduce_multicollinearity(X)

# Train and evaluate the model
final_model = train_and_evaluate(X, y)

import pandas as pd
from sklearn.preprocessing import StandardScaler # type: ignore

# Fit the StandardScaler to your original data (X) outside the function
scaler = StandardScaler()
scaler.fit(X)  # X is your original feature dataframe

# Assuming 'final_model' is your trained stacking model
# and 'X' is your dataframe with features
def predict_sale_price(new_data):
  """
  Predicts the sale price of a property using the trained model.

  Args:
    new_data: A Pandas DataFrame containing the features of the property
              to predict the sale price for.

  Returns:
    The predicted sale price of the property.
  """

  # Standardize features using the global scaler
  new_data_scaled = scaler.transform(new_data)

  # Predict using the final model
  prediction = final_model.predict(new_data_scaled)

  return prediction[0]

# Create a new dataframe with features for prediction
new_data = pd.DataFrame({
    'Estimated Value': [500000],  # Replace with your actual values
    'Locality': [0],  # Replace with encoded values for locality
    'num_bathrooms': [2],
    'Residential': [0], # Replace with encoded values for Residential
    'property_tax_rate': [0.01]
})

# Make prediction
prediction = predict_sale_price(new_data)
print("Predicted Sale Price:", prediction)

